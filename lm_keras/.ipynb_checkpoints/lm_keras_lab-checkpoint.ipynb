{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig(object):\n",
    "    def __init__(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = 20\n",
    "        self.EMBEDDING_DIM = 100\n",
    "        self.VALIDATION_SPLIT = 0.2\n",
    "        self.batch_size = 20\n",
    "        self.units = 200\n",
    "        self.nb_words = 9999\n",
    "        self.nb_time_steps = 20\n",
    "        self.nb_input_vector = 1\n",
    "        self.sgd_lr = 1.0\n",
    "        self.sgd_momentum = 0.9\n",
    "        self.sgd_decay = 0.0\n",
    "        self.nb_epoch = 3\n",
    "        self.steps_per_epoch = 25\n",
    "\n",
    "\n",
    "m_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Py3 = sys.version_info[0] == 3\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        if Py3:\n",
    "            return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "        else:\n",
    "            return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def cut_num_step(train_data, num_step):\n",
    "    x = []\n",
    "    y = []\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in range(len(train_data)):\n",
    "        if i % num_step == 0 and i != 0:\n",
    "            x.append(a)\n",
    "            y.append(b)\n",
    "            a = []\n",
    "            b = []\n",
    "        if i == len(train_data) - 1:\n",
    "            b.append(0)\n",
    "        else:\n",
    "            a.append(train_data[i])\n",
    "            b.append(to_categorical(train_data[i + 1], num_classes=10000))\n",
    "    return np.asarray(x), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self):\n",
    "        self.config = m_config\n",
    "\n",
    "    def build_lm_model(self, inputs_x, targets):\n",
    "        inputs = Input(tensor=inputs_x.get_next())\n",
    "        embedding_layer = Embedding(m_config.nb_words + 1,\n",
    "                                    m_config.EMBEDDING_DIM,\n",
    "                                    input_length=m_config.MAX_SEQUENCE_LENGTH)\n",
    "        lstm_layer = LSTM(\n",
    "            m_config.units,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='hard_sigmoid',\n",
    "            use_bias=True,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            return_sequences=True)\n",
    "        x = embedding_layer(inputs)\n",
    "        x = lstm_layer(x)\n",
    "        x = lstm_layer(x)\n",
    "        predictions = Dense(m_config.nb_words + 1, activation='softmax')(x)\n",
    "        sgd = SGD(\n",
    "            lr=m_config.sgd_lr,\n",
    "            momentum=m_config.sgd_momentum,\n",
    "            decay=m_config.sgd_decay,\n",
    "            nesterov=False)\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=sgd,\n",
    "            metrics=['accuracy'],\n",
    "            target_tensors=[\n",
    "                targets.get_next()])\n",
    "        history = model.fit(\n",
    "            epochs=m_config.nb_epoch,\n",
    "            steps_per_epoch=m_config.steps_per_epoch)\n",
    "        ppl = np.exp(np.array(history.history[\"loss\"]))\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dta_path = os.getcwd() + \"/simple-examples/data/\"\n",
    "    train_data, valid_data, test_data, _ = ptb_raw_data(dta_path)\n",
    "#     x_train, y_train = cut_num_step(\n",
    "#         train_data[0:10001], m_config.nb_time_steps)\n",
    "    x_train, y_train = cut_num_step(\n",
    "        train_data, m_config.nb_time_steps)\n",
    "    features = x_train.astype('float32')\n",
    "    labels = y_train.astype('float32')\n",
    "    dataset_x = tf.data.Dataset.from_tensor_slices(features).repeat()\n",
    "    dataset_y = tf.data.Dataset.from_tensor_slices(labels).repeat()\n",
    "    dataset_x = dataset_x.batch(m_config.batch_size)\n",
    "    dataset_y = dataset_y.batch(m_config.batch_size)\n",
    "    itera_x = dataset_x.make_one_shot_iterator()\n",
    "    itera_y = dataset_y.make_one_shot_iterator()\n",
    "    train_model = LanguageModel()\n",
    "    print(\"ppl:\", train_model.build_lm_model(itera_x, itera_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-fisrt-week",
   "language": "python",
   "name": "first-week"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
